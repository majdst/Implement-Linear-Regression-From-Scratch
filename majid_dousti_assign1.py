# -*- coding: utf-8 -*-
"""Majid_Dousti_Assign1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Vo-_FQkhRIaL07D2xV20mnkh6yFdttNX

$Question\ 1 - a$

$Linear\ Regression\ with\ one\ variable $

$ Predict:\ motor-UPDRS\ < dependent> using\ PPE\ Feature\ <independent> $
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Reading the dataset
dataset = pd.read_csv("/content/parkinsons_dataset.csv")
dataset.sample(2)

#Our data set contains 5875 rows and 22 columns

"""$Assigning\ data\ to\ two\ different\ parts:$

$Training\qquad and\qquad Test$
"""

# set the random seed for reprocibility
seed = 1

# Shuffle the DataFrame rows without setting a seed
shuffled_dataset = dataset.sample(frac=1, random_state=seed)


#70% training and 30% testing
split_index = int(0.7 * len(shuffled_dataset))

#spliting data
train_dataset = shuffled_dataset.iloc[: split_index]
test_dataset = shuffled_dataset.iloc[split_index :]

print(f"number of whole dataset rows is: {len(dataset)}")
print(f"number of train rows is: {len(train_dataset)}")
print(f"number of test rows is: {len(test_dataset)}")

"""$Introducing\ the\ Dependent\ and\ Independent\ Feature$

$Training\ Set$
"""

# PPE as Independent

ppe_train = train_dataset['PPE']

#Let's take a look at this data length to be sure
print(len(ppe_train))

#Let's take a look at PPE
ppe_train.sample(5)

# motor_UPDRS as dependent

motor_UPDRS_train = train_dataset['motor_UPDRS']

#Let's take a look at this data length to be sure
print(len(motor_UPDRS_train))

#Let's take a look at PPE
#motor_UPDRS_test.describe()
motor_UPDRS_train.sample(5)

"""$Visual\ the\ data\ to\ have\ vison$"""

plt.xlim(0,1)
plt.ylim(0,50)
#plt.figure(figsize=(16,8))
plt.scatter(ppe_train, motor_UPDRS_train, color= 'r')
plt.xlabel("ppe_train")
plt.ylabel("motor_UPDRS_train")
plt.show()

"""$Gradient\ Descent $

$ (Predicted - Y)\ $ $\hat{y} = ax + b $

$  ------------------------ $

$Sum\ of\ Squared\ Residual\ <R^2>\ :$

$R^2 = ∑(Y(Real\ Data)\ - (intercept\ <b> + slope\ <a>*X(Real\ Data)))^2$

$ Loss\ Function = \frac{R^2}{n} $

$ ------------------------ $

$Derivative\ againts\ intercept\ <b> :$

$ d/db\ (L) = \frac{∑(-2 * (Y\ - (b + a*X)))}{n} $

$ ------------------------ $

$Derivative\ againts\ intercept\ <a> :$

$ d/da\ (L) = \frac{∑(-2 * X * (Y\ - (b + a*X)))}{n} $

$ ------------------------ $


$Stepsize = d/db\ L(first\ x) * Learning\ rate (0.1\ Usually) $

$New\ Intercept <b> = old\ Intercept - stepsize$

$ ------------------------ $

$NOTE:\ Gradient\ Descent\ stops\ when\ the\ step\ size\ <slope * Learning\ rate> is\ very\ close\ to\ 0 $

$ ------------------------ $



"""

# Number of Train Data
n_train = train_dataset.shape[0]
print(f"number of Training dataset: {n_train}")

# First b
b = 10
# First a
a = 10
# Iteration
iteration = 10
learning_rate = 0.1
# to store the trend of the derivative agianst a
slope_trend = []

for i in range (iteration):

  lossfunction =  sum (( motor_UPDRS_train - (b + a*ppe_train))**2)/n_train
  # Deravitave a
  derivative_a = sum(-2*ppe_train*(motor_UPDRS_train - (b + a*ppe_train)))/n_train
  # Deravitive b
  derivative_b = sum(-2*(motor_UPDRS_train - (b + a*ppe_train)))/n_train
  slope_trend.append(lossfunction)

  print(f" Iteration {i+1}, slope a = {derivative_a}, slope b = {derivative_b}  loss = {lossfunction}")

  if abs(derivative_a) < 10**-6 or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
    break

  else:
    step_a = derivative_a * learning_rate
    a = a - step_a
    step_b = derivative_b * learning_rate
    b = b - step_b



#derivativeb is slope of (sum squared residual vs intercept)
print(a)
print(b)
#plot the trend of difference between iteration and derivativea
plt.plot(range((i+1) ,0, -1), slope_trend[::-1], marker = "o" )
plt.xlabel('Iteration')
plt.ylabel('lossfunction')
plt.title('train data')

""" # After 10 Iteration, The lost function is like above. ---> need more iteration to be plato#"""

# First b
b = 10
# First a
a = 10
# Iteration
iteration = 20
learning_rate = 0.1
# to store the trend of the derivative agianst a
slope_trend = []

for i in range (iteration):

  lossfunction =  sum (( motor_UPDRS_train - (b + a*ppe_train))**2)/n_train
  # Deravitave a
  derivative_a = sum(-2*ppe_train*(motor_UPDRS_train - (b + a*ppe_train)))/n_train
  # Deravitive b
  derivative_b = sum(-2*(motor_UPDRS_train - (b + a*ppe_train)))/n_train
  slope_trend.append(lossfunction)

  print(f" Iteration {i+1}, slope a = {derivative_a}, slope b = {derivative_b}  loss = {lossfunction}")

  if abs(derivative_a) < 10**-6 or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
    break
  else:
    step_a = derivative_a * learning_rate
    a = a - step_a
    step_b = derivative_b * learning_rate
    b = b - step_b




#derivativeb is slope of (sum squared residual vs intercept)
print(a)
print(b)
#plot the trend of difference between iteration and derivativea
plt.plot(range((i+1) ,0, -1), slope_trend[::-1], marker = "o" )
plt.xlabel('Iteration')
plt.ylabel('lossfunction')
plt.title('train data')

"""# After 20 Iteration, The lost function is like above. ---> need more iteration to be plato#"""

# First b
b = 10
# First a
a = 10
# Iteration
iteration = 30
learning_rate = 0.1
# to store the trend of the derivative agianst a
slope_trend = []

for i in range (iteration):

  lossfunction =  sum (( motor_UPDRS_train - (b + a*ppe_train))**2)/n_train
  # Deravitave a
  derivative_a = sum(-2*ppe_train*(motor_UPDRS_train - (b + a*ppe_train)))/n_train
  # Deravitive b
  derivative_b = sum(-2*(motor_UPDRS_train - (b + a*ppe_train)))/n_train
  slope_trend.append(lossfunction)

  print(f" Iteration {i+1}, slope a = {derivative_a}, slope b = {derivative_b}  loss = {lossfunction}")

  if abs(derivative_a) < 10**-6 or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
    break
  else:
    step_a = derivative_a * learning_rate
    a = a - step_a
    step_b = derivative_b * learning_rate
    b = b - step_b




#derivativeb is slope of (sum squared residual vs intercept)
print(a)
print(b)
#plot the trend of difference between iteration and derivativea
plt.plot(range((i+1) ,0, -1), slope_trend[::-1], marker = "o" )
plt.xlabel('Iteration')
plt.ylabel('lossfunction')
plt.title('train data')

"""#The Ideal one is b = 12.088 and a = 18.677#

$ Y = 12.088 + 18.677 * X $

#Testing our Model with Test Data
"""

# Number of Test Data
n_test = test_dataset.shape[0]
print(f"number of Training dataset: {n_test}")

ppe_test = test_dataset['PPE']

#Let's take a look at this data length to be sure
print(len(ppe_test))

#Let's take a look at PPE
ppe_test.sample(5)

# motor_UPDRS as dependent

motor_UPDRS_test = test_dataset['motor_UPDRS']

#Let's take a look at this data length to be sure
print(len(motor_UPDRS_test))

#Let's take a look at PPE
#motor_UPDRS_test.describe()
motor_UPDRS_test.sample(5)

"""#Train and Test Simultaneously for Iteration 25"""

# First b
b = 10
# First a
a = 10
# Iteration
iteration = 25
learning_rate = 0.1
# to store the trend of the derivative agianst a
slope_trend = []
# Store the trend of training losses
loss_train_trend = []
# Store the trend of testing losses
loss_test_trend = []

for i in range (iteration):


  #Loss_test
  loss_test =  sum (( motor_UPDRS_test - (b + a*ppe_test))**2)/n_test
  #Loss_Train
  loss_train = sum (( motor_UPDRS_train - (b + a*ppe_train))**2)/n_train
  # Deravitave a
  derivative_a = sum(-2*ppe_train*(motor_UPDRS_train - (b + a*ppe_train)))/n_train
  # Deravitive b
  derivative_b = sum(-2*(motor_UPDRS_train - (b + a*ppe_train)))/n_train

  slope_trend.append(loss_train)
  loss_train_trend.append(loss_train)
  loss_test_trend.append(loss_test)

  print(f" Iteration {i+1}, slope a = {derivative_a}, slope b = {derivative_b}  loss_train = {loss_train}, loss_test = {loss_test}")

  if abs(derivative_a) < 10**-6 or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
    break
  else:
    step_a = derivative_a * learning_rate
    a = a - step_a
    step_b = derivative_b * learning_rate
    b = b - step_b




#derivativeb is slope of (sum squared residual vs intercept)
print(f"a is: {a}")
print(f"b is: {b}")


# Plot for Training Loss
plt.subplot(1, 2, 1)
plt.plot(range(i + 1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss")
plt.xlabel('Iteration')
plt.ylabel('Loss Function')
plt.title('Training Loss Trend')

# Plot for Test Loss
plt.subplot(1, 2, 2)
plt.plot(range(i + 1, 0, -1), loss_test_trend[::-1], marker="o", label="Test Loss")
plt.xlabel('Iteration')
plt.ylabel('Loss Function')
plt.title('Test Loss Trend')


plt.tight_layout()  # Adjust layout for better spacing
plt.show()

# Independent Feature ppe-test
 # Dependent Feature motor_UPDRS_test

#Number of Training Set
number_set = test_dataset.shape[0]
print(f"Total number Testing data: {number_set}")
print("  ")


# Motor_UPDRS_test
motor_UPDRS_test_bar = np.sum(motor_UPDRS_test) / number_set
print(f"Average motor_UPDRS_test is: {motor_UPDRS_test_bar}")
print("  ")

MSE = (1/number_set) * np.sum( (motor_UPDRS_test - (16.924 + 19.372 * ppe_test))**2 )
print(f"MSE is: {MSE}")
print("  ")

#Sum Squared Regression (SSR):
ssr = np.sum((motor_UPDRS_test - (16.924 + 19.372*ppe_test))**2)
print(f"SSR is: {ssr}")
print("  ")

# Total Sum of Squares (SST):
sst = np.sum( (motor_UPDRS_test - motor_UPDRS_test_bar)**2 )
print(f"SST is: {sst}")

r_squared = 1 - (ssr / sst)
print(f" R^2 is : {r_squared} ")

"""#R^2 equals 1% which describe that 1% reduction in variation once we took in ppe took into account #
#The greater the R^2, better the model#

$Adjusted\ R^2$

#The adjusted R-squared value will always be lower than the R-squared value when unnecessary variables are added to the model. This makes it a useful metric for comparing models with different numbers of variables, helping to identify whether the additional variables contribute significantly to the explanatory power of the model.
"""

#k = number of parameters in mean value: ------> straight line  ---> 1
k = 1
n = number_set

#p_value = 1 - ( ((1-R_squared)*(n-1)) / (n-k-1) )
#print(f"adjusted r squared: {p_value}")

"""If R^2 and adjusted R^2 are very close, it suggests that the inclusion of additional predictors in the model has not significantly affected the adjusted
R^2. This could mean that the added predictors are relevant and contribute meaningfully to explaining the variance in the dependent variable. However, it's important to consider other factors, such as the overall fit of the model, the significance of individual predictors, and potential issues like multicollinearity.
In summary, when R^2 and adjusted R^2 are close, it may indicate that the additional predictors in the model are providing meaningful explanatory power without being penalized much by the adjustment for the number of predictors.

$End\ of\ Part\ 1$\
$Part\ 2$

$Question\ 2 $

$Linear\ Regression\ with\ two\ variables $

$ Predict:\ motor-UPDRS\ < dependent> using\ PPE\ and\ NHR\ Features\ <independents> $
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#Readinng the dataset
dataset = pd.read_csv("/content/parkinsons_dataset.csv")
dataset.sample(2)

"""$Assigning\ data\ to\ two\ different\ parts:$

$Training\qquad and\qquad Test$
"""

# set the random seed for reprocibility
seed = 1

# Shuffle the DataFrame rows without setting a seed
shuffled_dataset = dataset.sample(frac=1, random_state=seed)


#70% training and 30% testing
split_index = int(0.7 * len(shuffled_dataset))

#spliting data
train_dataset = shuffled_dataset.iloc[: split_index]
test_dataset = shuffled_dataset.iloc[split_index :]

print(f"number of whole dataset rows is: {len(dataset)}")
print(f"number of train rows is: {len(train_dataset)}")
print(f"number of train rows is: {len(test_dataset)}")

"""$Introducing\ the\ Dependent\ and\ Independent\ Features$

$Training\ Set$
"""

# PPE as Independent

ppe_train = train_dataset['PPE']

#Let's take a look at this data length to be sure
print(len(ppe_train))

#Let's take a look at PPE
ppe_train.sample(5)

# NHR as Independent

NHR_train = train_dataset['NHR']

#Let's take a look at this data length to be sure
print(len(NHR_train))

#Let's take a look at PPE
NHR_train.sample(5)

# motor_UPDRS as dependent

motor_UPDRS_train = train_dataset['motor_UPDRS']

#Let's take a look at this data length to be sure
print(len(motor_UPDRS_train))

#motor_UPDRS_test.describe()
motor_UPDRS_train.sample(5)

plt.xlim(0,1)
plt.ylim(0,50)
#plt.figure(figsize=(16,8))
plt.scatter(ppe_train, motor_UPDRS_train, color= 'm')
plt.xlabel("ppe_train")
plt.ylabel("motor_UPDRS_train")

plt.show()

import plotly.express as py
py.scatter_3d(x=ppe_train, y = NHR_train, z=  motor_UPDRS_train)

plt.xlim(-.1,1)
plt.ylim(0,50)
#plt.figure(figsize=(16,8))
plt.scatter(NHR_train, motor_UPDRS_train, color= 'r')
plt.xlabel("NHR_train")
plt.ylabel("motor_UPDRS_train")

plt.show()

"""$Gradient\ Descent $

$ (Predicted - Y)\ $ $\hat{y} = a_1X + a_2X + b $


$ --------------------- $

$Sum\ of\ Squared\ Residual\ <R^2>\ :$


$R^2 = ∑(Y(Real\ Data)\ - (intercept\ <b> + slope1\ <a1>*X(Real\ Data\ ppe) + slope2\ <a2>*X(Real\ Data\ NHR)))^2$

$Loss\ Function = \frac{R^2}{n} $

$ --------------------- $

$Derivative\ againts\ intercept\ <a> :$

$ --------------------- $

$ d/da_1\ (L) = -2 * (X1) * (Y\ - (b + a1*X1 + a2*X2)) $

$ --------------------- $

$ d/da_2\ (L) = -2 * (X2) * (Y\ - (b + a1*X1 + a2*X2)) $

$ --------------------- $

$ d/db\ (L) = -2 * (Y\ - (b + a1*X1 + a2*X2)) $

$ ---------------------- $

$New\ Intercept <a> = old\ Intercept - stepsize$

$ ---------------------- $
"""

# Number of Train Data
n_train = train_dataset.shape[0]
print(f"number of Training dataset: {n_train}")

# First b
b = 10
# First a
a_1 = 10
# First a
a_2 = 10
# Iteration
iteration = 10
learning_rate = 0.1
# to store the trend of the derivative agianst a
slope_trend = []

for i in range (iteration):

  lossfunction =  sum (( motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train))**2)/n_train

  # Deravitave a_1
  derivative_a1 = sum(-2*ppe_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitave a_2
  derivative_a2 = sum(-2*NHR_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitive b
  derivative_b = sum(-2*(motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  slope_trend.append(lossfunction)

  print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope b = {derivative_b}  loss = {lossfunction}")

  if abs(derivative_a1) < 10**-6 or abs(derivative_a2) < 10**-6 or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
    break

  else:
    step_a1 = derivative_a1 * learning_rate
    a_1 = a_1 - step_a1

    step_a2 = derivative_a2 * learning_rate
    a_2 = a_2 - step_a2

    step_b = derivative_b * learning_rate
    b = b - step_b



#derivativeb is slope of (sum squared residual vs intercept)
print(a_1)
print(a_2)
print(b)

#plot the trend of difference between iteration and derivativea
plt.plot(range((i+1) ,0, -1), slope_trend[::-1], marker = "o" )
plt.xlabel('Iteration')
plt.ylabel('lossfunction')
plt.title('train data')

"""# After 10 Iteration, The lost function is like above. ---> need more iteration to be plato#"""

# First b
b = 10
# First a
a_1 = 10
# First a
a_2 = 10
# Iteration
iteration = 15
learning_rate = 0.1
# to store the trend of the derivative agianst a
slope_trend = []

for i in range (iteration):

  lossfunction =  sum (( motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train))**2)/n_train

  # Deravitave a_1
  derivative_a1 = sum(-2*ppe_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitave a_2
  derivative_a2 = sum(-2*NHR_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitive b
  derivative_b = sum(-2*(motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  slope_trend.append(lossfunction)

  print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope b = {derivative_b}  loss = {lossfunction}")

  if abs(derivative_a1) < 10**-6 or abs(derivative_a2) < 10**-6 or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
    break

  else:
    step_a1 = derivative_a1 * learning_rate
    a_1 = a_1 - step_a1

    step_a2 = derivative_a2 * learning_rate
    a_2 = a_2 - step_a2

    step_b = derivative_b * learning_rate
    b = b - step_b



#derivativeb is slope of (sum squared residual vs intercept)
print(a_1)
print(a_2)
print(b)

#plot the trend of difference between iteration and derivativea
plt.plot(range((i+1) ,0, -1), slope_trend[::-1], marker = "o" )
plt.xlabel('Iteration')
plt.ylabel('lossfunction')
plt.title('train data')

"""# After 15 Iteration, The lost function is like above. ---> need more iteration to be plato#"""

# First b
b = 10
# First a
a_1 = 10
# First a
a_2 = 10
# Iteration
iteration = 20
learning_rate = 0.1
# to store the trend of the derivative agianst a
slope_trend = []

for i in range (iteration):

  lossfunction =  sum (( motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train))**2)/n_train

  # Deravitave a_1
  derivative_a1 = sum(-2*ppe_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitave a_2
  derivative_a2 = sum(-2*NHR_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitive b
  derivative_b = sum(-2*(motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  slope_trend.append(lossfunction)

  print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope b = {derivative_b}  loss = {lossfunction}")

  if abs(derivative_a1) < 10**-6 or abs(derivative_a2) < 10**-6 or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
    break

  else:
    step_a1 = derivative_a1 * learning_rate
    a_1 = a_1 - step_a1

    step_a2 = derivative_a2 * learning_rate
    a_2 = a_2 - step_a2

    step_b = derivative_b * learning_rate
    b = b - step_b



#derivativeb is slope of (sum squared residual vs intercept)
print(f"a_1 is: {a_1}")
print(f"a_2 is: {a_2}")
print(f"b is: {b}")

#plot the trend of difference between iteration and derivativea
plt.plot(range((i+1) ,0, -1), slope_trend[::-1], marker = "o" )
plt.xlabel('Iteration')
plt.ylabel('lossfunction')
plt.title('train data')

# First b
b = 10
# First a
a_1 = 10
# First a
a_2 = 10
# Iteration
iteration = 25
learning_rate = 0.1
# to store the trend of the derivative agianst a
slope_trend = []

for i in range (iteration):

  lossfunction =  sum (( motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train))**2)/n_train

  # Deravitave a_1
  derivative_a1 = sum(-2*ppe_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitave a_2
  derivative_a2 = sum(-2*NHR_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitive b
  derivative_b = sum(-2*(motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  slope_trend.append(lossfunction)

  print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope b = {derivative_b}  loss = {lossfunction}")

  if abs(derivative_a1) < 10**-6 or abs(derivative_a2) < 10**-6 or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
    break

  else:
    step_a1 = derivative_a1 * learning_rate
    a_1 = a_1 - step_a1

    step_a2 = derivative_a2 * learning_rate
    a_2 = a_2 - step_a2

    step_b = derivative_b * learning_rate
    b = b - step_b



#derivativeb is slope of (sum squared residual vs intercept)
print(f"a_1 is: {a_1}")
print(f"a_2 is: {a_2}")
print(f"b is: {b}")

#plot the trend of difference between iteration and derivativea
plt.plot(range((i+1) ,0, -1), slope_trend[::-1], marker = "o" )
plt.xlabel('Iteration')
plt.ylabel('lossfunction')
plt.title('train data')

"""# After 25 Iteration, The lost function is like above. ---> no more iteration#

#Y = 18.3310 + 11.837 * X1(PPE) + 10.134 * X2(NHR)

#Testing our Model with Test Data
"""

# Number of Test Data
n_test = test_dataset.shape[0]
print(f"number of Training dataset: {n_test}")

ppe_test = test_dataset['PPE']

#Let's take a look at this data length to be sure
print(len(ppe_test))

#Let's take a look at PPE
ppe_test.sample(5)

NHR_test = test_dataset['PPE']

#Let's take a look at this data length to be sure
print(len(NHR_test))

#Let's take a look at PPE
NHR_test.sample(5)

# motor_UPDRS as dependent

motor_UPDRS_test = test_dataset['motor_UPDRS']

#Let's take a look at this data length to be sure
print(len(motor_UPDRS_test))

#Let's take a look at PPE
#motor_UPDRS_test.describe()
motor_UPDRS_test.sample(5)

""":# Train and Test Simultaneously for Iteration 25"""

# First b
b = 10
# First a
a_1 = 10
# First a
a_2 = 10
# Iteration
iteration = 50
learning_rate = 0.1
# to store the trend of the derivative agianst a
slope_trend = []
# Store the trend of training losses
loss_train_trend = []
# Store the trend of testing losses
loss_test_trend = []

for i in range (iteration):


  # loss test
  loss_test =  sum (( motor_UPDRS_test - (b + a_1*ppe_test + a_2*NHR_test))**2)/n_test

  # loss_train
  loss_train =  sum (( motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train))**2)/n_train

  # Deravitave a_1
  derivative_a1 = sum(-2*ppe_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitave a_2
  derivative_a2 = sum(-2*NHR_train * (motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  # Deravitive b
  derivative_b = sum(-2*(motor_UPDRS_train - (b + a_1*ppe_train + a_2*NHR_train)))/n_train

  slope_trend.append(loss_train)
  loss_train_trend.append(loss_train)
  loss_test_trend.append(loss_test)

  print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope b = {derivative_b},  loss_train = {loss_train}, loss_test = {loss_test}")

  if abs(derivative_a1) < 10**-6 or abs(derivative_a2) < 10**-6 or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
    break

  else:
    step_a1 = derivative_a1 * learning_rate
    a_1 = a_1 - step_a1

    step_a2 = derivative_a2 * learning_rate
    a_2 = a_2 - step_a2

    step_b = derivative_b * learning_rate
    b = b - step_b



#derivativeb is slope of (sum squared residual vs intercept)
print(f"a_1 is: {a_1}")
print(f"a_2 is: {a_2}")
print(f"b is: {b}")

# Plot for Training Loss
plt.subplot(1, 2, 1)
#plt.figure(figsize = (16,8))
plt.plot(range(i + 1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss")
plt.xlabel('Iteration')
plt.ylabel('Loss Function')
plt.title('Training Loss Trend')

# Plot for Test Loss
plt.subplot(1, 2, 2)
#plt.figure(figsize = (16,8))
plt.plot(range(i + 1, 0, -1), loss_test_trend[::-1], marker="o", label="Test Loss")
plt.xlabel('Iteration')
plt.ylabel('Loss Function')
plt.title('Test Loss Trend')


plt.tight_layout()  # Adjust layout for better spacing
plt.show()

"""# Converge in 25 iteration which in Comparison to Q1, 5 steps easier.
# Loss function with 2 features is 20.33 which in Comparison to Q1 with Loss Function = 67.11, which should said NHR works pretty well in our model.
"""

# Independent Feature ppe-test
# Indepenedent Feature NHR
 # Dependent Feature motor_UPDRS_test

 #Number of Training Set
number_set = test_dataset.shape[0]
print(f"Total number Testing data: {number_set}")
print("  ")

# Motor_UPDRS_test
motor_UPDRS_test_bar = np.sum(motor_UPDRS_test) / number_set
print(f"Average motor_UPDRS_test is: {motor_UPDRS_test_bar}")
print("  ")

MSE = (1/number_set) * np.sum( (motor_UPDRS_test - (16.7349 + 11.3146 * ppe_test + 9.6116 * NHR_test))**2 )
print(f"MSE is: {MSE}")
print("  ")

#Sum Squared Regression (SSR):
ssr = np.sum((motor_UPDRS_test - (16.7349 + 11.3146 * ppe_test + 9.6116 * NHR_test))**2)
print(f"SSR is: {ssr}")
print("  ")

# Total Sum of Squares (SST):
sst = np.sum( (motor_UPDRS_test - motor_UPDRS_test_bar)**2 )
print(f"SST is: {sst}")
print("  ")

r_squared = 1 - (ssr / sst)
print(f"R^2 is : {r_squared} ")

"""$End\ of\ part\ 2$

$Part\ 3$

$Forward \ Stepwise\ Linear\ Regression\  $

$ Predict:\ motor-UPDRS\ < dependent> using\ RPDE\ ,\ Shimmer: APQ5\ , \ DFA\ , \ HNR \ , \ Shimmer(dB)\  Features\ <independents> $
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Reading the dataset
dataset = pd.read_csv("/content/parkinsons_dataset.csv")
dataset.sample(2)

"""
$Assigning\ data\ to\ two\ different\ parts:$

$Training\qquad and\qquad Test$
#This is formatted as code"""

# set the random seed for reprocibility
seed = 1

# Shuffle the DataFrame rows without setting a seed
shuffled_dataset = dataset.sample(frac=1, random_state=seed)


#70% training and 30% testing
split_index = int(0.7 * len(shuffled_dataset))

#spliting data
train_dataset = shuffled_dataset.iloc[: split_index]
test_dataset = shuffled_dataset.iloc[split_index :]

print(f"number of whole dataset rows is: {len(dataset)}")
print(f"number of train rows is: {len(train_dataset)}")
print(f"number of train rows is: {len(test_dataset)}")

#1
motor_UPDRS_train = train_dataset ['motor_UPDRS']
motor_UPDRS_test = test_dataset ['motor_UPDRS']
#2
rpde_train = train_dataset['RPDE']
rpde_test = test_dataset['RPDE']
#3
shimmer_APQ5_train = train_dataset['Shimmer:APQ5']
shimmer_APQ5_test = test_dataset['Shimmer:APQ5']
#4
dfa_train = train_dataset['DFA']
dfa_test = test_dataset['DFA']
#5
nhr_train = train_dataset['NHR']
nhr_test = test_dataset['NHR']
#6
Shimmer_dB_train = train_dataset['Shimmer(dB)']
Shimmer_dB_test = test_dataset['Shimmer(dB)']

#7
test_time_train = train_dataset['test_time']
test_time_test = test_dataset['test_time']


#8

age_train = train_dataset['age']
age_test = test_dataset['age']

#9
shimmer_APQ11_train = train_dataset['Shimmer:APQ11']
shimmer_APQ11_test = test_dataset['Shimmer:APQ11']
#10
ppe_train = train_dataset['PPE']
ppe_test = test_dataset['PPE']
#11
shimmer_train = train_dataset['Shimmer']
shimmer_test = test_dataset['Shimmer']

# Plot for rpde_train
plt.subplot(2, 1, 1)
plt.scatter(rpde_train	, motor_UPDRS_train, color= 'r')
plt.xlabel("rpde_train")
plt.ylabel("motor_UPDRS_train")

plt.show()

# Plot for Train shimmer_APQ5_train
plt.subplot(2, 1, 1)
plt.scatter(shimmer_APQ5_train	, motor_UPDRS_train, color= 'b')
plt.xlabel("shimmer:APQ5_train")
plt.ylabel("motor_UPDRS_train")

plt.show()

# Plot for Train DFA_train
plt.subplot(2, 1, 1)
plt.scatter(dfa_train	, motor_UPDRS_train, color= 'm')
plt.xlabel("dfa_train")
plt.ylabel("motor_UPDRS_train")

plt.show()

# Plot for Train DFA_train
plt.subplot(2, 1, 1)
plt.scatter(nhr_train	, motor_UPDRS_train, color= 'y')
plt.xlabel("nhr_train")
plt.ylabel("motor_UPDRS_train")

plt.show()

# Plot for Train DFA_train
plt.subplot(2, 1, 1)
plt.scatter(Shimmer_dB_train	, motor_UPDRS_train, color= 'c')
plt.xlabel("Shimmer_dB_train")
plt.ylabel("motor_UPDRS_train")

plt.show()

# Number of Train and Test Data
n_train = train_dataset.shape[0]
n_test = test_dataset.shape[0]
print(f"number of Training dataset: {n_train}")
print(f"number of Test dataset: {n_test}")

"""#Function for Training Part"""

def gradient_descent(b, a1, X1, X1_t, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*X1_t ))**2)/n_test
    loss_train =  sum (( motor_UPDRS_train - (b + a1*X1 ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*X1 * (motor_UPDRS_train - (b + a1*X1)))/n_train

    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b + a1*X1)))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

def MSE(b, a1, X1_t ):
  # Motor_UPDRS_test:
  #Mean Square Error:
  MSE = (1/n_test) * np.sum( (motor_UPDRS_test - (b + a1 * X1_t))**2 )
  print(f"MSE is: {MSE}")
  print("  ")

gradient_descent(b = 10, a1 = 10, X1 = rpde_train, X1_t = rpde_test, iteration = 15, learning_rate = 0.1)
MSE(b = 14.5, a1 = 12.43, X1_t = rpde_test )

"""$Step\ I-2$

#shimmer_APQ5 va Motor_UPDRS
"""

gradient_descent(b = 10, a1 = 10, X1 = shimmer_APQ5_train, X1_t = shimmer_APQ5_test, iteration = 20, learning_rate = 0.1)
MSE(b = 20.99, a1 = 10.26, X1_t = shimmer_APQ5_test )

"""$Step\ I-3$

#dfa va Motor_UPDRS
"""

gradient_descent(b = 10, a1 = 10, X1 = dfa_train, X1_t = dfa_test, iteration = 20, learning_rate = 0.1)
MSE(b = 13.53, a1 = 11.82, X1_t = dfa_test )

"""$Step\ I-4$

#nhr va Motor_UPDRS
"""

gradient_descent(b = 10, a1 = 10, X1 = nhr_train, X1_t = nhr_test, iteration = 20, learning_rate = 0.1)
MSE(b = 20.87, a1 = 10.35, X1_t = nhr_test)

"""$Step\ I-5$

#Shimmer_dB va Motor_UPDRS
"""

gradient_descent(b = 10, a1 = 10, X1 = Shimmer_dB_train, X1_t = Shimmer_dB_test, iteration = 20, learning_rate = 0.1)
MSE(b = 17.77, a1 = 10.88, X1_t = nhr_test)

"""# Based on MSE (The smaller the MSE, the better the model's predictive accuracy), I choose the RPDE"""

# Function for two Variable
# RPDE is choosen

def gradient_descent(b, a1, a2, X2, X2_t, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*X2_t ))**2)/n_test
    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*X2 ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*X2)))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*X2 * (motor_UPDRS_train - (b + a1*rpde_train + a2*X2)))/n_train

    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b + a1*rpde_train + a2*X2)))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a1 = {derivative_a2}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

# Function for MSE
def MSE(b, a1, a2, X2_t ):
  # Motor_UPDRS_test
  # b, a1, and a2 from final tested model
  MSE = (1/n_test) * np.sum( (motor_UPDRS_test - (b + a1 * rpde_test + a2 * X2_t))**2 )
  print(f"MSE is: {MSE}")
  print("  ")

"""$Stepz\ II-1$
# shimmer_APQ5 vs RPDE - Motor_UPDRS
"""

gradient_descent(b=10, a1 = 10, a2 = 10 , X2 = shimmer_APQ5_train, X2_t = shimmer_APQ5_test, iteration = 20, learning_rate = 0.1)
MSE(b=14.42, a1=12.325, a2=10.094, X2_t = shimmer_APQ5_test )

"""$Stepz\ II-2$
# DFA vs RPDE - Motor_UPDRS
"""

gradient_descent(b=10, a1 = 10, a2 = 10 , X2 = dfa_train, X2_t = dfa_test, iteration = 20, learning_rate = 0.1)
MSE(b=9.803, a1=9.881, a2=9.371, X2_t = dfa_test )

"""$Stepz\ II-3$
#NHR vs RPDE - Motor_UPDRS
"""

gradient_descent(b=10, a1 = 10, a2 = 10 , X2 = nhr_train, X2_t = nhr_test, iteration = 20, learning_rate = 0.1)
MSE(b=14.359, a1=12.222, a2=10.030, X2_t = nhr_test )

"""$Stepz\ II-4$
#Shimmer(dB) vs RPDE - Motor_UPDRS
"""

gradient_descent(b=10, a1 = 10, a2 = 10 , X2 = Shimmer_dB_train, X2_t = Shimmer_dB_test, iteration = 40, learning_rate = 0.1)
MSE(b=12.95, a1=10.808, a2=7.769, X2_t = Shimmer_dB_test )

# Function for three Variable
# shimmer_APQ5 and RPDE is choosen

def gradient_descent(b, a1, a2, a3, X3, X3_t, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  a3
  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a3*X3_t ))**2)/n_test
    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*X3 ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*X3)))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*X3)))/n_train

    #   Deravitave a_3
    derivative_a3 = sum(-2* X3 * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*X3)))/n_train

    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*X3)))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a3 = {derivative_a3}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      step_a3 = derivative_a3 * learning_rate
      a3 = a3 - step_a3

      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  print(f"a3 is: {a3}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

# Function for MSE
def MSE(b, a1, a2, a3, X3_t ):
  # Motor_UPDRS_test
  # b, a1, a2 and a3 from final tested model
  MSE = (1/n_test) * np.sum( (motor_UPDRS_test - (b + a1 * rpde_test + a2*shimmer_APQ5_test + a3 * X3_t))**2 )
  print(f"MSE is: {MSE}")
  print("  ")

"""# Base on the MSE, shimmer_APQ5 is choosen

$Stepz\ III-1$
#DFA vs shimmer_APQ5 - RPDE - Motor_UPDRS
"""

gradient_descent(b = 10, a1 = 10, a2 = 10, a3 = 10, X3 = dfa_train, X3_t = dfa_test, iteration = 20, learning_rate = 0.1)
MSE(b = 9.6952, a1 = 9.7953, a2 = 9.999, a3 = 9.2963, X3_t = dfa_test )

"""$Stepz\ III-2$
#NHR vs shimmer_APQ5 - RPDE - Motor_UPDRS
"""

gradient_descent(b = 10, a1 = 10, a2 = 10, a3 = 10, X3 = nhr_train, X3_t = nhr_test, iteration = 20, learning_rate = 0.1)
MSE(b = 14.2128, a1 = 12.1156, a2 = 10.0584, a3 = 9.9947, X3_t = nhr_test )

"""$Stepz\ III-3$
#Shimmer(dB) vs shimmer_APQ5 - RPDE - Motor_UPDRS
"""

gradient_descent(b = 10, a1 = 10, a2 = 10, a3 = 10, X3 = Shimmer_dB_train, X3_t = Shimmer_dB_test, iteration = 70, learning_rate = 0.1)
MSE(b = 13.4816, a1 = 10.5729, a2 = 9.6778, a3 = 5.9056, X3_t = nhr_test )

"""# By comparing MSE from this three variables, NHR is choosen for next round of code"""

# Function for three Variable
# shimmer_APQ5 and RPDE is choosen

def gradient_descent(b, a1, a2, a3, a4, X4, X4_t, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  a3
  # Forth a
  a4
  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a3*nhr_test + a4 * X4_t ))**2)/n_test
    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * X4 ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * X4)))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * X4)))/n_train

    #   Deravitave a_3
    derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * X4)))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* X4 * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * X4)))/n_train

    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * X4)))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a3 = {derivative_a3}, slope a4 = {derivative_a4}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      step_a3 = derivative_a3 * learning_rate
      a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

# Function for MSE
def MSE(b, a1, a2, a3, a4, X4_t ):
  # Motor_UPDRS_test
  # b, a1, a2 and a3 from final tested model
  MSE = (1/n_test) * np.sum( (motor_UPDRS_test - (b + a1 * rpde_test + a2*shimmer_APQ5_test + a3 * nhr_test + a4 * X4_t))**2 )
  print(f"MSE is: {MSE}")
  print("  ")

"""$Step\ IV-1$
#DFA vs NHR - shimmer_APQ5 - RPDE - Motor_UPDRS
"""

gradient_descent(b = 10, a1 = 10, a2 =10, a3 =10, a4 = 10, X4 = dfa_train, X4_t = dfa_test, iteration = 15, learning_rate = 0.1)
MSE(b = 9.2844, a1 = 8.6179, a2 = 9.9701, a3 = 9.8976, a4 = 9.2844, X4_t = dfa_test )

"""$Step\ IV-2$
#Shimmer(dB) vs NHR - shimmer_APQ5 - RPDE - Motor_UPDRS
"""

gradient_descent(b = 10, a1 = 10, a2 =10, a3 =10, a4 = 10, X4 = Shimmer_dB_train, X4_t = Shimmer_dB_test, iteration = 150, learning_rate = 0.1)
MSE(b = 14.7219, a1 = 10.1134, a2 = 9.3701, a3 = 7.6428, a4 = 2.0858, X4_t = Shimmer_dB_test )

"""$Step\ V-1$
#Shimmer(dB) vs DFA- NHR - shimmer_APQ5 - RPDE - Motor_UPDRS
"""

# Function for four Variable
# DFA, NHR, shimmer_APQ5 and RPDE are choosen

def gradient_descent(b, a1, a2, a3, a4, a5, X5, X5_t, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  a3
  # Forth a
  a4
  # Fifth a
  a5
  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a3*nhr_test + a4 * dfa_test + a5 * X5_t ))**2)/n_test
    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train + a5 * X5 ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train + a5 * X5)))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + dfa_train + a5 * X5)))/n_train

    #   Deravitave a_3
    derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train + a5 * X5)))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train + a5 * X5)))/n_train

    #   Deravitave a_5
    derivative_a5 = sum(-2* X5 * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train + a5 * X5)))/n_train

    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train + a5 * X5)))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a3 = {derivative_a3}, slope a4 = {derivative_a4}, slope a5 = {derivative_a5}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      step_a3 = derivative_a3 * learning_rate
      a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5

      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

# Function for MSE
def MSE(b, a1, a2, a3, a4, a5, X5_t ):
  # Motor_UPDRS_test
  # b, a1, a2, a3, a4 and a5 from final tested model
  MSE = (1/n_test) * np.sum( (motor_UPDRS_test - (b + a1 * rpde_test + a2*shimmer_APQ5_test + a3 * nhr_test + a4 * dfa_train + a5 * X5_t))**2 )
  print(f"MSE is: {MSE}")
  print("  ")

gradient_descent(b = 10, a1 = 10, a2 = 10, a3 = 10, a4 = 10, a5 = 10, X5 = Shimmer_dB_train, X5_t = Shimmer_dB_test, iteration = 15, learning_rate = 0.1)
MSE(b = 8.0962, a1 = 8.6103, a2 = 10.1529, a3 = 9.5751, a4 = 8.3421, a5 = 7.8618, X5_t = Shimmer_dB_test )

"""# Final Model from Forward step size linear regression:
# Y = 8.096230084760895 + 8.610332162289275 * X(rpde) + 10.152944995345264 * X(shimmer_APQ5) + 9.575147049011013 * X(NHR) + 8.342166327039113 * X(DFA) + 7.861858176505558 * X(shimmer_dB)

$Backward \ Stepwise\ Linear\ Regression\  $

$ Predict:\ motor-UPDRS\ < dependent>\

$ \ using\ ,\ all\ 10\ features \ <independents> $
"""

# Function for four Variable
#rpde - shimmer shimmer_APQ5 - nhr - dfa - Shimmer_dB - shimmer_APQ11 - ppe - shimmer - test_time - age

def gradient_descent(b, a1, a2, a3, a4, a5, a6, a7, a8,a9 ,a10, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  a3
  # Forth a
  a4
  # Fifth a
  a5
  # 6th a
  a6
   # 7th a
  a7
   # 8th a
  a8
  # 9th a
  a9
  # 10th a
  a10
  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a3*nhr_test + a4 * dfa_test + \
                                           a5 * Shimmer_dB_test +a6 * shimmer_APQ11_test+ a7* ppe_test + a8 * shimmer_test + a9 * test_time_test + a10 * age_test ))**2)/n_test

    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train)))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_3
    derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train)))/n_train

    # Deravitave a_5
    derivative_a5 = sum(-2* Shimmer_dB_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_6
    derivative_a6 = sum(-2* shimmer_APQ11_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

     #   Deravitave a_7
    derivative_a7 = sum(-2* ppe_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_8
    derivative_a8 = sum(-2* shimmer_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_9
    derivative_a9 = sum(-2* test_time_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_10
    derivative_a10 = sum(-2* age_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train
    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train)))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a3 = {derivative_a3}, slope a4 = {derivative_a4}, \
     slope a5 = {derivative_a5}, slope a6 = {derivative_a6}, slope a7 = {derivative_a7}, slope a8 = {derivative_a8}, \
      slope a9 = {derivative_a9}, slope a10 = {derivative_a10}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      step_a3 = derivative_a3 * learning_rate
      a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5

      step_a6 = derivative_a6 * learning_rate
      a6 = a6 - step_a6

      step_a7 = derivative_a7 * learning_rate
      a7 = a7 - step_a7

      step_a8 = derivative_a8 * learning_rate
      a8 = a8 - step_a8

      step_a9 = derivative_a9 * learning_rate
      a9 = a9 - step_a9

      step_a10 = derivative_a10 * learning_rate
      a10 = a10 - step_a10


      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  print(f"a6 is: {a6}")
  print(f"a7 is: {a7}")
  print(f"a8 is: {a8}")
  print(f"a9 is: {a9}")
  print(f"a10 is: {a10}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

# Function for MSE
def MSE(b, a1, a2, a3, a4, a5, a6, a7, a8, a9, a10 ):
  # Motor_UPDRS_test
  # b, a1, a2, a3, a4 and a5 from final tested model

  MSE = (1/n_test) * np.sum( (motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a3*nhr_test + a4 * dfa_test + \
                              a5 * Shimmer_dB_test +a6 * shimmer_APQ11_test+ a7* ppe_test + a8 * shimmer_test + a9 * test_time_test + a10 * age_test))**2 )
  print(f"MSE is: {MSE}")
  print("  ")

gradient_descent(b=10, a1=10, a2=10, a3=10, a4=10, a5=10, a6=10, a7=10, a8=10,a9=10 ,a10=10, iteration=10, learning_rate=0.1)
MSE(b=5.5459, a1=2.9945 , a2=1.1077, a3=1.7396, a4=3.6269, a5=1.7140, a6=1.5151, a7=1.2165, a8=1.8733, a9=6.3597, a10=3.6215 )

"""$Step\ I-1$
# The Mse is too large ----> Look at the age and Test_Time in comparison to other data are way larger, At first step, I eliminate the Test time
"""

# Function for four Variable
#rpde - shimmer shimmer_APQ5 - nhr - dfa - Shimmer_dB - shimmer_APQ11 - ppe - shimmer - test_time - age

def gradient_descent(b, a1, a2, a3, a4, a5, a6, a7, a8, a10 , iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  a3
  # Forth a
  a4
  # Fifth a
  a5
  # 6th a
  a6
   # 7th a
  a7
  # 8th
  a8

  # 9th a
  #a9

  # 10th a
  a10
  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a3*nhr_test + a4 * dfa_test + \
                                           a5 * Shimmer_dB_test +a6 * shimmer_APQ11_test+ a7* ppe_test + a8 * shimmer_test + a10 * age_test ))**2)/n_test

    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train)))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train

    #   Deravitave a_3
    derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  +a10 * age_train)))/n_train

    # Deravitave a_5
    derivative_a5 = sum(-2* Shimmer_dB_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train

    #   Deravitave a_6
    derivative_a6 = sum(-2* shimmer_APQ11_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train

     #   Deravitave a_7
    derivative_a7 = sum(-2* ppe_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train

    #   Deravitave a_8
    derivative_a8 = sum(-2* shimmer_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  +a10 * age_train )))/n_train

    #   Deravitave a_9
    #derivative_a9 = sum(-2* test_time_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
    #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_10
    derivative_a10 = sum(-2* age_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train
    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train)))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a3 = {derivative_a3}, slope a4 = {derivative_a4}, \
     slope a5 = {derivative_a5}, slope a6 = {derivative_a6}, slope a7 = {derivative_a7}, slope a8 = {derivative_a8}, slope a10 = {derivative_a10}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      step_a3 = derivative_a3 * learning_rate
      a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5

      step_a6 = derivative_a6 * learning_rate
      a6 = a6 - step_a6

      step_a7 = derivative_a7 * learning_rate
      a7 = a7 - step_a7

      step_a8 = derivative_a8 * learning_rate
      a8 = a8 - step_a8

      #step_a9 = derivative_a9 * learning_rate
      #a9 = a9 - step_a9

      step_a10 = derivative_a10 * learning_rate
      a10 = a10 - step_a10


      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  print(f"a6 is: {a6}")
  print(f"a7 is: {a7}")
  print(f"a8 is: {a8}")
  #print(f"a9 is: {a9}")
  print(f"a10 is: {a10}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

gradient_descent(b=10, a1=10, a2=10, a3=10, a4=10, a5=10, a6=10, a7=10, a8=10, a10=10 , iteration=10, learning_rate=0.1)

"""$Step\ I-2$
#In this level, the loss function decreased but it is still too high. Now, age will be removed for next step
"""

# Function for four Variable
#rpde - shimmer shimmer_APQ5 - nhr - dfa - Shimmer_dB - shimmer_APQ11 - ppe - shimmer - test_time - age

def gradient_descent(b, a1, a2, a3, a4, a5, a6, a7, a8, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  a3
  # Forth a
  a4
  # Fifth a
  a5
  # 6th a
  a6
   # 7th a
  a7
   # 8th a
  a8

  # 9th a
  #a9

  # 10th a
  #a10

  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a3*nhr_test + a4 * dfa_test + \
                                           a5 * Shimmer_dB_test +a6 * shimmer_APQ11_test+ a7* ppe_test + a8 * shimmer_test ))**2)/n_test

    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train )))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_3
    derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    # Deravitave a_5
    derivative_a5 = sum(-2* Shimmer_dB_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_6
    derivative_a6 = sum(-2* shimmer_APQ11_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

     #   Deravitave a_7
    derivative_a7 = sum(-2* ppe_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_8
    derivative_a8 = sum(-2* shimmer_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_9
    #derivative_a9 = sum(-2* test_time_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
    #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_10
    #derivative_a10 = sum(-2* age_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train
    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train )))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a3 = {derivative_a3}, slope a4 = {derivative_a4}, \
     slope a5 = {derivative_a5}, slope a6 = {derivative_a6}, slope a7 = {derivative_a7}, slope a8 = {derivative_a8}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      step_a3 = derivative_a3 * learning_rate
      a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5

      step_a6 = derivative_a6 * learning_rate
      a6 = a6 - step_a6

      step_a7 = derivative_a7 * learning_rate
      a7 = a7 - step_a7

      step_a8 = derivative_a8 * learning_rate
      a8 = a8 - step_a8

      #step_a9 = derivative_a9 * learning_rate
      #a9 = a9 - step_a9

      #step_a10 = derivative_a10 * learning_rate
      #a10 = a10 - step_a10


      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  print(f"a6 is: {a6}")
  print(f"a7 is: {a7}")
  print(f"a8 is: {a8}")
  #print(f"a9 is: {a9}")
  #print(f"a10 is: {a10}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

gradient_descent(b=10, a1=10, a2=10, a3=10, a4=10, a5=10, a6=10, a7=10, a8=10 , iteration=70, learning_rate=0.1)
MSE(b=8.8851, a1=7.5610, a2=9.4391, a3=8.2397, a4=7.1735, a5=2.422, a6=9.4252, a7=7.9182, a8=9.1334, a9=0, a10=0 )

"""# Removing that two features really was good choice. Now, I will put a1...  a8 zero and check the MSE.
#Any which has highest MSE, will be removed.
"""

MSE(b=8.8851, a1=7.5610, a2=9.4391, a3=8.2397, a4=7.1735, a5=2.422, a6=9.4252, a7=7.9182, a8=9.1334, a9=0, a10=0 )

"""#MSE when ai become 0
#[87.534,71.102,70.34,91.847,70.7324,71.199,73.4297,70.9721,]
#MSE of our Model with 8 features is 71.5483.
#The item which by elimination, the mse doesn't change meaningfully, should be removed
#shimmer: APQ11 will be removed. a6

$Step\ I-3$
"""

# Function for four Variable
#rpde - shimmer_APQ5 - nhr - dfa - Shimmer_dB - shimmer_APQ11 - ppe - shimmer - test_time - age

def gradient_descent(b, a1, a2, a3, a4, a5, a7, a8, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  a3
  # Forth a
  a4
  # Fifth a
  a5
  # 6th a
  #a6
   # 7th a
  a7
   # 8th a
  a8

  # 9th a
  #a9

  # 10th a
  #a10

  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a3*nhr_test + a4 * dfa_test + \
                                           a5 * Shimmer_dB_test + a7* ppe_test + a8 * shimmer_test ))**2)/n_test

    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train )))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_3
    derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    # Deravitave a_5
    derivative_a5 = sum(-2* Shimmer_dB_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_6
    #derivative_a6 = sum(-2* shimmer_APQ11_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

     #   Deravitave a_7
    derivative_a7 = sum(-2* ppe_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_8
    derivative_a8 = sum(-2* shimmer_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_9
    #derivative_a9 = sum(-2* test_time_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
    #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_10
    #derivative_a10 = sum(-2* age_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train
    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train )))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a3 = {derivative_a3}, slope a4 = {derivative_a4}, \
     slope a5 = {derivative_a5}, slope a7 = {derivative_a7}, slope a8 = {derivative_a8}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      step_a3 = derivative_a3 * learning_rate
      a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5

      #step_a6 = derivative_a6 * learning_rate
      #a6 = a6 - step_a6

      step_a7 = derivative_a7 * learning_rate
      a7 = a7 - step_a7

      step_a8 = derivative_a8 * learning_rate
      a8 = a8 - step_a8

      #step_a9 = derivative_a9 * learning_rate
      #a9 = a9 - step_a9

      #step_a10 = derivative_a10 * learning_rate
      #a10 = a10 - step_a10


      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  #print(f"a6 is: {a6}")
  print(f"a7 is: {a7}")
  print(f"a8 is: {a8}")
  #print(f"a9 is: {a9}")
  #print(f"a10 is: {a10}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

gradient_descent(b=10, a1=10, a2=10, a3=10, a4=10, a5=10, a7=10, a8=10, iteration=100, learning_rate=0.1)

"""#Backward in comparison to forward converged slowly"""

#MSE After 3 feature Elimination
MSE(b=9.7007, a1=7.605, a2=9.3529, a3=7.8897, a4=6.905, a5=1.3743, a6=0, a7=7.7811, a8=9.007, a9=0, a10=0 )

"""#Comparing when one feature is zero with MSE After 3"""

MSE(b=9.7007, a1=7.605, a2=9.3529, a3=7.8897, a4=6.905, a5=1.3743, a6=0, a7=7.7811, a8=9.007, a9=0, a10=0 )

"""#[86.5, 70.0196,69.4895,88.859, 69.9087, 72.4439, 69.948 ]

\# Comparison between MSE after 3 and next MSE shows
by 0 ing the a3(nhr), the MSE decreased which is awesome

$Step\ I-4$
"""

# Function for four Variable
#rpde - shimmer_APQ5 - nhr - dfa - Shimmer_dB - shimmer_APQ11 - ppe - shimmer - test_time - age

def gradient_descent(b, a1, a2, a4, a5, a7, a8, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  #a3
  # Forth a
  a4
  # Fifth a
  a5
  # 6th a
  #a6
   # 7th a
  a7
   # 8th a
  a8

  # 9th a
  #a9

  # 10th a
  #a10

  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a4 * dfa_test + \
                                           a5 * Shimmer_dB_test + a7* ppe_test + a8 * shimmer_test ))**2)/n_test

    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train )))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_3
    #derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    # Deravitave a_5
    derivative_a5 = sum(-2* Shimmer_dB_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_6
    #derivative_a6 = sum(-2* shimmer_APQ11_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

     #   Deravitave a_7
    derivative_a7 = sum(-2* ppe_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_8
    derivative_a8 = sum(-2* shimmer_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_9
    #derivative_a9 = sum(-2* test_time_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
    #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_10
    #derivative_a10 = sum(-2* age_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train
    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train )))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a4 = {derivative_a4}, \
     slope a5 = {derivative_a5}, slope a7 = {derivative_a7}, slope a8 = {derivative_a8}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      #step_a3 = derivative_a3 * learning_rate
      #a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5

      #step_a6 = derivative_a6 * learning_rate
      #a6 = a6 - step_a6

      step_a7 = derivative_a7 * learning_rate
      a7 = a7 - step_a7

      step_a8 = derivative_a8 * learning_rate
      a8 = a8 - step_a8

      #step_a9 = derivative_a9 * learning_rate
      #a9 = a9 - step_a9

      #step_a10 = derivative_a10 * learning_rate
      #a10 = a10 - step_a10


      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  #print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  #print(f"a6 is: {a6}")
  print(f"a7 is: {a7}")
  print(f"a8 is: {a8}")
  #print(f"a9 is: {a9}")
  #print(f"a10 is: {a10}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

gradient_descent(b=10, a1=10, a2=10, a4=10, a5=10, a7=10, a8=10, iteration=45, learning_rate=0.1)

#MSE After 4 feature Elimination
MSE(b=8.1169, a1=7.8665, a2=9.6432, a3=0, a4=7.3811, a5=5.1213, a6=0, a7=8.4953, a8=9.4437, a9=0, a10=0 )

#MSE for Comparison with 4 feature Elimination
MSE(b=8.1169, a1=7.8665, a2=9.6432, a3=0, a4=7.3811, a5=5.1213, a6=0, a7=8.4953, a8=9.4437, a9=0, a10=0 )

"""#[89.254, 71.387, 93.4416,71.9878, 74.0987, 71.234 ]

# Comparison between MSE after 4 (71.86223) and 0 ones[],
#(a8) Shimmer should be removed.
"""

# Function for four Variable
#rpde a1 - shimmer_APQ5 a2 - nhr a3 - dfa a4 - Shimmer_dB a5 - shimmer_APQ11 a6 - ppe a7 - shimmer a8 - test_time a9 - age a10

def gradient_descent(b, a1, a2, a4, a5, a7, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  #a3
  # Forth a
  a4
  # Fifth a
  a5
  # 6th a
  #a6
   # 7th a
  a7
   # 8th a
  #a8

  # 9th a
  #a9

  # 10th a
  #a10

  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a4 * dfa_test + \
                                           a5 * Shimmer_dB_test + a7* ppe_test ))**2)/n_test

    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train )))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train

    #   Deravitave a_3
    #derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train

    # Deravitave a_5
    derivative_a5 = sum(-2* Shimmer_dB_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train )))/n_train

    #   Deravitave a_6
    #derivative_a6 = sum(-2* shimmer_APQ11_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

     #   Deravitave a_7
    derivative_a7 = sum(-2* ppe_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train  )))/n_train

    #   Deravitave a_8
    #derivative_a8 = sum(-2* shimmer_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                    #          + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_9
    #derivative_a9 = sum(-2* test_time_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
    #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_10
    #derivative_a10 = sum(-2* age_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train
    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a4 = {derivative_a4}, \
     slope a5 = {derivative_a5}, slope a7 = {derivative_a7}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      #step_a3 = derivative_a3 * learning_rate
      #a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5

      #step_a6 = derivative_a6 * learning_rate
      #a6 = a6 - step_a6

      step_a7 = derivative_a7 * learning_rate
      a7 = a7 - step_a7

      #step_a8 = derivative_a8 * learning_rate
      #a8 = a8 - step_a8

      #step_a9 = derivative_a9 * learning_rate
      #a9 = a9 - step_a9

      #step_a10 = derivative_a10 * learning_rate
      #a10 = a10 - step_a10


      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  #print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  #print(f"a6 is: {a6}")
  print(f"a7 is: {a7}")
  #print(f"a8 is: {a8}")
  #print(f"a9 is: {a9}")
  #print(f"a10 is: {a10}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

gradient_descent(b = 10, a1 = 10, a2 = 10, a4 = 10, a5 = 10, a7 = 10, iteration=45, learning_rate = 0.1)

#MSE After 5 feature Elimination
MSE(b=8.1963, a1=7.9786, a2=9.6724, a3=0, a4=7.4432, a5=5.5292, a6=0, a7=8.5974, a8=0, a9=0, a10=0 )

"""$Final\ Model\ for\ Backward\ stepwise\ linear\ regression$
#Y = 8.1963 + 7.9786X1(RPDE) + 9.6724X2(shimmer_APQ5) + 7.4432X4(DFA) + 5.5292X5(shimmer_dB) + 8.5974X7(PPE)

$Question\ 3\ c$
# I think the forward model works better than backward based on the convergence ability. It is converged way faster with less steps.
Badan bia MSE final Model ham moghaise kon

$Final\ Model\ for\ Backward\ stepwise\ linear\ regression$
#Y = 8.1963 + 7.9786X1(RPDE) + 9.6724X2(shimmer_APQ5) + 7.4432X4(DFA) + 5.5292X5(shimmer_dB) + 8.5974X7(PPE)

$ Final\ Model\ from\ Forward\ stepwise\ linear\ regression$
# Y = 8.096230084760895 + 8.610332162289275 * X(rpde) + 10.152944995345264 * X(shimmer_APQ5) + 9.575147049011013 * X(NHR) + 8.342166327039113 * X(DFA) + 7.861858176505558 * X(shimmer_dB)

Y = 8.096230084760895 + 8.610332162289275 * X(rpde) + 10.152944995345264 * X(shimmer_APQ5) + 9.575147049011013 * X(NHR) + 8.342166327039113 * X(DFA) + 7.861858176505558 * X(shimmer_dB)

$End\ of\ Part\ 3$

$Part\ 4$
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd


# Reading the dataset
dataset = pd.read_csv("/content/parkinsons_dataset.csv")
dataset.sample(2)

"""$Assigning\ data\ to\ two\ different\ parts:$

$Training\qquad and\qquad Test$# This is formatted as code
"""

# set the random seed for reprocibility
seed = 1

# Shuffle the DataFrame rows without setting a seed
shuffled_dataset = dataset.sample(frac=1, random_state=seed)


#70% training and 30% testing
split_index = int(0.7 * len(shuffled_dataset))

#spliting data
train_dataset = shuffled_dataset.iloc[: split_index]
test_dataset = shuffled_dataset.iloc[split_index :]

print(f"number of whole dataset rows is: {len(dataset)}")
print(f"number of train rows is: {len(train_dataset)}")
print(f"number of train rows is: {len(test_dataset)}")

#1
motor_UPDRS_train = train_dataset ['motor_UPDRS']
motor_UPDRS_test = test_dataset ['motor_UPDRS']
#2
rpde_train = train_dataset['RPDE']
rpde_test = test_dataset['RPDE']
#3
shimmer_APQ5_train = train_dataset['Shimmer:APQ5']
shimmer_APQ5_test = test_dataset['Shimmer:APQ5']
#4
dfa_train = train_dataset['DFA']
dfa_test = test_dataset['DFA']
#5
nhr_train = train_dataset['NHR']
nhr_test = test_dataset['NHR']
#6
Shimmer_dB_train = train_dataset['Shimmer(dB)']
Shimmer_dB_test = test_dataset['Shimmer(dB)']
#7
test_time_train = train_dataset['test_time']
test_time_test = test_dataset['test_time']
#8
age_train = train_dataset['age']
age_test = test_dataset['age']
#9
shimmer_APQ11_train = train_dataset['Shimmer:APQ11']
shimmer_APQ11_test = test_dataset['Shimmer:APQ11']
#10
ppe_train = train_dataset['PPE']
ppe_test = test_dataset['PPE']
#11
shimmer_train = train_dataset['Shimmer']
shimmer_test = test_dataset['Shimmer']

# Number of Train and Test Data
n_train = train_dataset.shape[0]
n_test = test_dataset.shape[0]
print(f"number of Training dataset: {n_train}")
print(f"number of Test dataset: {n_test}")

"""#From Q3, I choose the Backward regression Model for regularization


#Y = 8.1963 + 7.9786X1(RPDE) + 9.6724X2(shimmer_APQ5) + 7.4432X4(DFA) + 5.5292X5(shimmer_dB) + 8.5974X7(PPE)
"""

# Function for four Variable
#rpde a1 - shimmer_APQ5 a2 - nhr a3 - dfa a4 - Shimmer_dB a5 - shimmer_APQ11 a6 - ppe a7 - shimmer a8 - test_time a9 - age a10

def gradient_descent(b, a1, a2, a4, a5, a7, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  #a3
  # Forth a
  a4
  # Fifth a
  a5
  # 6th a
  #a6
   # 7th a
  a7
   # 8th a
  #a8

  # 9th a
  #a9

  # 10th a
  #a10

  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []



  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a4 * dfa_test + \
                                           a5 * Shimmer_dB_test + a7* ppe_test ))**2)/n_test

    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train )))/n_train

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train

    #   Deravitave a_3
    #derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train

    # Deravitave a_5
    derivative_a5 = sum(-2* Shimmer_dB_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train )))/n_train

    #   Deravitave a_6
    #derivative_a6 = sum(-2* shimmer_APQ11_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

     #   Deravitave a_7
    derivative_a7 = sum(-2* ppe_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train  )))/n_train

    #   Deravitave a_8
    #derivative_a8 = sum(-2* shimmer_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                    #          + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_9
    #derivative_a9 = sum(-2* test_time_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
    #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_10
    #derivative_a10 = sum(-2* age_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train
    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a4 = {derivative_a4}, \
     slope a5 = {derivative_a5}, slope a7 = {derivative_a7}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      #step_a3 = derivative_a3 * learning_rate
      #a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5

      #step_a6 = derivative_a6 * learning_rate
      #a6 = a6 - step_a6

      step_a7 = derivative_a7 * learning_rate
      a7 = a7 - step_a7

      #step_a8 = derivative_a8 * learning_rate
      #a8 = a8 - step_a8

      #step_a9 = derivative_a9 * learning_rate
      #a9 = a9 - step_a9

      #step_a10 = derivative_a10 * learning_rate
      #a10 = a10 - step_a10


      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  #print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  #print(f"a6 is: {a6}")
  print(f"a7 is: {a7}")
  #print(f"a8 is: {a8}")
  #print(f"a9 is: {a9}")
  #print(f"a10 is: {a10}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

"""$Lasso\ Regularization$

# Regularizaed Loss = Loss + (Lambda * ∑abs(ai))
"""

# Function for four Variable
#rpde a1 - shimmer_APQ5 a2 - nhr a3 - dfa a4 - Shimmer_dB a5 - shimmer_APQ11 a6 - ppe a7 - shimmer a8 - test_time a9 - age a10

def gradient_descent(b, a1, a2, a4, a5, a7, iteration, learning_rate, Lambda, alpha):

  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []
  # Store the trend of training losses - regularized
  loss_train_trend_r = []
  # Store the trend of testing losses - regularized
  loss_test_trend_r = []




  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a4 * dfa_test + \
                                           a5 * Shimmer_dB_test + a7* ppe_test ))**2)/n_test

    #Regularized Loss
    rLoss_test = loss_test + (Lambda * sum([a1, a2 , a4 ,a5, a7]))

    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train ))**2)/n_train
    #Regularized Loss
    rloss_train = loss_train + (Lambda * sum([a1, a2 , a4 ,a5, a7]))


    # Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train )))/n_train
    # Regularized Deravite a_1
    rderivative_a1 = derivative_a1 + Lambda

    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train

    # Regularized Deravite a_2
    rderivative_a2 = derivative_a2 + Lambda

    #   Deravitave a_3
    #derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train

    # Regularized Deravite a_4
    rderivative_a4 = derivative_a4 + Lambda

    # Deravitave a_5
    derivative_a5 = sum(-2* Shimmer_dB_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train )))/n_train

    # Regularized Deravite a_5
    rderivative_a5 = derivative_a5 + Lambda

    #   Deravitave a_6
    #derivative_a6 = sum(-2* shimmer_APQ11_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

     #   Deravitave a_7
    derivative_a7 = sum(-2* ppe_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train  )))/n_train

    # Regularized Deravite a_7
    rderivative_a7 = derivative_a7 + Lambda


    #   Deravitave a_8
    #derivative_a8 = sum(-2* shimmer_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                    #          + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_9
    #derivative_a9 = sum(-2* test_time_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
    #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_10
    #derivative_a10 = sum(-2* age_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train
    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)
    loss_train_trend_r.append(rloss_train)
    loss_test_trend_r.append(rLoss_test)

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a4 = {derivative_a4}, \
     slope a5 = {derivative_a5}, slope a7 = {derivative_a7}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    print("")

    print(f" Iteration {i+1}, reg-slope a1 = {rderivative_a1}, reg-slope a2 = {rderivative_a2}, reg-slope a4 = {rderivative_a4}, \
     reg-slope a5 = {rderivative_a5}, reg-slope a7 = {rderivative_a7}, reg-loss-train = {rloss_train}, reg-loss-test = {rLoss_test}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1
      #regularized a
      ra1 = a1 - step_a1 - (alpha * step_a1)

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2
      #regularized a
      ra2 = a2 - step_a2 - (alpha * step_a2)

      #step_a3 = derivative_a3 * learning_rate
      #a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4
      #regularized a
      ra4 = a4 - step_a4 - (alpha * step_a4)

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5
      #regularized a
      ra5 = a5 - step_a5 - (alpha * step_a5)

      #step_a6 = derivative_a6 * learning_rate
      #a6 = a6 - step_a6

      step_a7 = derivative_a7 * learning_rate
      a7 = a7 - step_a7
      #regularized a
      ra7 = a7 - step_a7 - (alpha * step_a7)

      #step_a8 = derivative_a8 * learning_rate
      #a8 = a8 - step_a8

      #step_a9 = derivative_a9 * learning_rate
      #a9 = a9 - step_a9

      #step_a10 = derivative_a10 * learning_rate
      #a10 = a10 - step_a10


      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  #print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  #print(f"a6 is: {a6}")
  print(f"a7 is: {a7}")
  #print(f"a8 is: {a8}")
  #print(f"a9 is: {a9}")
  #print(f"a10 is: {a10}")
  print(f"b is: {b}")
  print("")
  print(f"ra1 is: {ra1}")
  print(f"ra2 is: {ra2}")
  print(f"ra4 is: {ra4}")
  print(f"ra5 is: {ra5}")
  print(f"ra7 is: {ra7}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend_r[::-1], marker="o", label="Training Loss regularized", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend_r[::-1], marker="o", label="Testing Loss regularized", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function regularized')
  plt.title('Training and Testing Loss regularized Trend')
  plt.legend()
  plt.show()

gradient_descent(b=10, a1=10, a2=10, a4=10, a5=10, a7=10, iteration=20, learning_rate=0.1, Lambda=1, alpha=0.1)

gradient_descent(b=10, a1=10, a2=10, a4=10, a5=10, a7=10, iteration=20, learning_rate=0.1, Lambda=0.1, alpha=0.1)

gradient_descent(b=10, a1=10, a2=10, a4=10, a5=10, a7=10, iteration=20, learning_rate=0.1, Lambda=10, alpha=0.1)

gradient_descent(b=10, a1=10, a2=10, a4=10, a5=10, a7=10, iteration=20, learning_rate=0.1, Lambda=100, alpha=0.1)

gradient_descent(b=10, a1=10, a2=10, a4=10, a5=10, a7=10, iteration=20, learning_rate=0.1, Lambda=1, alpha=0.01)

gradient_descent(b=10, a1=10, a2=10, a4=10, a5=10, a7=10, iteration=20, learning_rate=0.1, Lambda=0.1, alpha=0.01)

"""$Feature\ Scaling$

#Normalization is good to use when the distribution of data does not follow a Gaussian distribution. It can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors.

# The general formula for normalization:

$\ x' = \frac{x - min(x)}{max(x) - min(x)}$
"""

#1
motor_UPDRS_train = train_dataset ['motor_UPDRS']

min_motor_UPDRS_train = np.min(motor_UPDRS_train)
max_motor_UPDRS_train = np.max(motor_UPDRS_train)
print(f"min_motor_UPDRS_train is {min_motor_UPDRS_train}, max_motor_UPDRS_train is {max_motor_UPDRS_train} ")
print("")
#Nomralization
motor_UPDRS_train_n = (motor_UPDRS_train - min_motor_UPDRS_train)/(max_motor_UPDRS_train - min_motor_UPDRS_train)

motor_UPDRS_test = test_dataset ['motor_UPDRS']

min_motor_UPDRS_test = np.min(motor_UPDRS_test)
max_motor_UPDRS_test = np.max(motor_UPDRS_test)
print(f"min_motor_UPDRS_test is {min_motor_UPDRS_test}, max_motor_UPDRS_test is {max_motor_UPDRS_test} ")
print("")
#Nomralization
motor_UPDRS_test_n = (motor_UPDRS_test - min_motor_UPDRS_test)/(max_motor_UPDRS_test - min_motor_UPDRS_test)

#2
rpde_train = train_dataset['RPDE']

min_rpde_train = np.min(rpde_train)
max_rpde_train = np.max(rpde_train)
print(f"min_rpde_train is {min_rpde_train}, max_rpde_train is {max_rpde_train} ")
print("")
#Nomralization
rpde_train_n = (rpde_train - min_rpde_train)/(max_rpde_train - min_rpde_train)

rpde_test = test_dataset['RPDE']

min_rpde_test = np.min(rpde_test)
max_rpde_test = np.max(rpde_test)
print(f"min_rpde_test is {min_rpde_train}, max_rpde_test is {max_rpde_test} ")
print("")
#Nomralization
rpde_test_n = (rpde_test - min_rpde_test)/(max_rpde_test - min_rpde_test)

#3

shimmer_APQ5_train = train_dataset['Shimmer:APQ5']

min_shimmer_APQ5_train = np.min(shimmer_APQ5_train)
max_shimmer_APQ5_train = np.max(shimmer_APQ5_train)

print(f"min_shimmer_APQ5_train is {min_shimmer_APQ5_train}, max_shimmer_APQ5_train is {max_shimmer_APQ5_train} ")
print("")
#Nomralization
shimmer_APQ5_train_n = (shimmer_APQ5_train - min_shimmer_APQ5_train)/(max_shimmer_APQ5_train - min_shimmer_APQ5_train)

shimmer_APQ5_test = test_dataset['Shimmer:APQ5']

min_shimmer_APQ5_test = np.min(shimmer_APQ5_test)
max_shimmer_APQ5_test = np.max(shimmer_APQ5_test)

print(f"min_shimmer_APQ5_test is {min_shimmer_APQ5_test}, max_shimmer_APQ5_test is {max_shimmer_APQ5_test}")
print("")
#Nomralization
shimmer_APQ5_test_n = (shimmer_APQ5_test - min_shimmer_APQ5_test)/(max_shimmer_APQ5_test - min_shimmer_APQ5_test)

#4
dfa_train = train_dataset['DFA']
min_dfa_train = np.min(dfa_train)
max_dfa_train= np.max(dfa_train)

min_dfa_train = np.min(dfa_train)
max_dfa_train = np.max(dfa_train)
print(f"min_dfa_train is {min_dfa_train}, max_dfa_train is {max_dfa_train}")
print("")
#Nomralization
dfa_train_n = (dfa_train - min_dfa_train)/(max_dfa_train - min_dfa_train)

dfa_test = test_dataset['DFA']

min_dfa_test = np.min(dfa_test)
max_dfa_test= np.max(dfa_test)

min_dfa_test = np.min(dfa_test)
max_dfa_test = np.max(dfa_test)
print(f"min_dfa_test is {min_dfa_test}, max_dfa_test is {max_dfa_test}")
print("")
#Nomralization
dfa_test_n = (dfa_test - min_dfa_test)/(max_dfa_test - min_dfa_test)


#5
nhr_train = train_dataset['NHR']

min_nhr_train = np.min(nhr_train)
max_nhr_train= np.max(nhr_train)

min_nhr_train = np.min(nhr_train)
max_nhr_train = np.max(nhr_train)
print(f"min_nhr_train is {min_nhr_train}, max_nhr_train is {max_nhr_train}")
print("")

nhr_test = test_dataset['NHR']

min_nhr_test = np.min(nhr_test)
max_nhr_test= np.max(nhr_test)

min_nhr_test = np.min(nhr_test)
max_nhr_test = np.max(nhr_test)
print(f"min_nhr_test is {min_nhr_test}, max_nhr_test is {max_nhr_test}")
print("")

#6
Shimmer_dB_train = train_dataset['Shimmer(dB)']

min_Shimmer_dB_train = np.min(Shimmer_dB_train)
max_Shimmer_dB_train= np.max(Shimmer_dB_train)

min_Shimmer_dB_train = np.min(Shimmer_dB_train)
max_Shimmer_dB_train = np.max(Shimmer_dB_train)
print(f"min_Shimmer_dB_train is {min_Shimmer_dB_train}, max_Shimmer_dB_train is {max_Shimmer_dB_train}")
print("")
#Nomralization
Shimmer_dB_train_n = (Shimmer_dB_train - min_Shimmer_dB_train)/(max_Shimmer_dB_train - min_Shimmer_dB_train)

Shimmer_dB_test = test_dataset['Shimmer(dB)']

min_Shimmer_dB_test = np.min(Shimmer_dB_test)
max_Shimmer_dB_test= np.max(Shimmer_dB_test)

min_Shimmer_dB_test = np.min(Shimmer_dB_test)
max_Shimmer_dB_test = np.max(Shimmer_dB_test)
print(f"min_Shimmer_dB_test is {min_Shimmer_dB_test}, max_Shimmer_dB_test is {max_Shimmer_dB_test}")
print("")
#Normalization
Shimmer_dB_test_n = (Shimmer_dB_test - min_Shimmer_dB_test)/(max_Shimmer_dB_test - min_Shimmer_dB_test)

#7
test_time_train = train_dataset['test_time']

test_time_test = test_dataset['test_time']

#8
age_train = train_dataset['age']
age_test = test_dataset['age']
#9
shimmer_APQ11_train = train_dataset['Shimmer:APQ11']
shimmer_APQ11_test = test_dataset['Shimmer:APQ11']
#10
ppe_train = train_dataset['PPE']
min_ppe_train = np.min(ppe_train)
max_ppe_train= np.max(ppe_train)

min_ppe_train = np.min(ppe_train)
max_ppe_train = np.max(ppe_train)
print(f"min_ppe_train is {min_ppe_train}, max_ppe_train is {max_ppe_train}")
print("")
#Nomralization
ppe_train_n = (ppe_train - min_ppe_train)/(max_ppe_train - min_ppe_train)

ppe_test = test_dataset['PPE']
min_ppe_test = np.min(ppe_test)
max_ppe_test= np.max(ppe_test)

min_ppe_test = np.min(ppe_test)
max_ppe_test = np.max(ppe_test)
print(f"min_ppe_test is {min_ppe_test}, max_ppe_test is {max_ppe_test}")
print("")
#Normalization
ppe_test_n = (ppe_test - min_ppe_test)/(max_ppe_test - min_ppe_test)
print(ppe_test_n)
print(ppe_test)

#11
shimmer_train = train_dataset['Shimmer']
shimmer_test = test_dataset['Shimmer']

# Function for four Variable
#rpde a1 - shimmer_APQ5 a2 - nhr a3 - dfa a4 - Shimmer_dB a5 - shimmer_APQ11 a6 - ppe a7 - shimmer a8 - test_time a9 - age a10

def gradient_descent(b, a1, a2, a4, a5, a7, iteration, learning_rate):
  # First b
  b
  # First a
  a1
  # Second a
  a2
  # Third a
  #a3
  # Forth a
  a4
  # Fifth a
  a5
  # 6th a
  #a6
   # 7th a
  a7
   # 8th a
  #a8

  # 9th a
  #a9

  # 10th a
  #a10

  # Iteration
  iteration
  learning_rate
  # to store the trend of the derivative agianst a
  slope_trend = []
  # Store the trend of training losses
  loss_train_trend = []
  # Store the trend of testing losses
  loss_test_trend = []
  # Store the trend of training losses normal
  loss_train_trend_n = []
  # Store the trend of testing losses normal
  loss_test_trend_n = []




  for i in range (iteration):

    loss_test = sum (( motor_UPDRS_test - (b + a1*rpde_test + a2*shimmer_APQ5_test + a4 * dfa_test + \
                                           a5 * Shimmer_dB_test + a7* ppe_test ))**2)/n_test

    #Normalized
    loss_test_n = sum (( motor_UPDRS_test_n - (b + a1*rpde_test_n + a2*shimmer_APQ5_test_n + a4 * dfa_test_n + \
                                           a5 * Shimmer_dB_test_n + a7* ppe_test_n ))**2)/n_test

    loss_train =  sum (( motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train ))**2)/n_train
    #Normalized
    loss_train_n =  sum (( motor_UPDRS_train_n - (b + a1*rpde_train_n + a2*shimmer_APQ5_train_n + a4 * dfa_train_n \
                                              + a5 * Shimmer_dB_train_n + a7* ppe_train_n ))**2)/n_train

    #   Deravitave a_1
    derivative_a1 = sum(-2*rpde_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train )))/n_train

    #   Deravitave a_1 normalized
    derivative_a1_n = sum(-2*rpde_train_n * (motor_UPDRS_train_n - (b + a1*rpde_train_n + a2*shimmer_APQ5_train_n + a4 * dfa_train_n \
                                              + a5 * Shimmer_dB_train_n +  a7* ppe_train_n )))/n_train



    #   Deravitave a_2
    derivative_a2 = sum(-2*shimmer_APQ5_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train

     #   Deravitave a_2_normailzed
    derivative_a2_n = sum(-2*shimmer_APQ5_train_n * (motor_UPDRS_train_n - (b + a1*rpde_train_n + a2*shimmer_APQ5_train_n + a4 * dfa_train_n \
                                              + a5 * Shimmer_dB_train_n + a7* ppe_train_n )))/n_train

    #   Deravitave a_3
    #derivative_a3 = sum(-2* nhr_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train +  a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_4
    derivative_a4 = sum(-2* dfa_train * (motor_UPDRS_train - (b + a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train

     #   Deravitave a_4 normalized
    derivative_a4_n = sum(-2* dfa_train_n * (motor_UPDRS_train_n - (b + a1*rpde_train_n + a2*shimmer_APQ5_train_n + a4 * dfa_train_n \
                                              + a5 * Shimmer_dB_train_n + a7* ppe_train_n )))/n_train

    # Deravitave a_5
    derivative_a5 = sum(-2* Shimmer_dB_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train )))/n_train

     # Deravitave a_5 normalized
    derivative_a5_n = sum(-2* Shimmer_dB_train_n * (motor_UPDRS_train_n - (b+ a1*rpde_train_n + a2*shimmer_APQ5_train_n + a4 * dfa_train_n \
                                              + a5 * Shimmer_dB_train_n +  a7* ppe_train_n )))/n_train

    #   Deravitave a_6
    #derivative_a6 = sum(-2* shimmer_APQ11_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

     #   Deravitave a_7
    derivative_a7 = sum(-2* ppe_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train +  a7* ppe_train  )))/n_train

     #   Deravitave a_7 normalized
    derivative_a7_n = sum(-2* ppe_train_n * (motor_UPDRS_train_n - (b+ a1*rpde_train_n + a2*shimmer_APQ5_train_n + a4 * dfa_train_n \
                                              + a5 * Shimmer_dB_train_n +  a7* ppe_train_n  )))/n_train

    #   Deravitave a_8
    #derivative_a8 = sum(-2* shimmer_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                    #          + a5 * Shimmer_dB_train + a7* ppe_train + a8 * shimmer_train  )))/n_train

    #   Deravitave a_9
    #derivative_a9 = sum(-2* test_time_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
    #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train + a9*test_time_train +a10 * age_train )))/n_train

    #   Deravitave a_10
    #derivative_a10 = sum(-2* age_train * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a3*nhr_train + a4 * dfa_train \
     #                                         + a5 * Shimmer_dB_train + a6 * shimmer_APQ11_train + a7* ppe_train + a8 * shimmer_train +a10 * age_train )))/n_train
    #   Deravitave b
    derivative_b = sum(-2 * (motor_UPDRS_train - (b+ a1*rpde_train + a2*shimmer_APQ5_train + a4 * dfa_train \
                                              + a5 * Shimmer_dB_train + a7* ppe_train )))/n_train

     #   Deravitave b normilized
    derivative_b_n = sum(-2 * (motor_UPDRS_train_n - (b+ a1*rpde_train_n + a2*shimmer_APQ5_train_n + a4 * dfa_train_n \
                                              + a5 * Shimmer_dB_train_n + a7* ppe_train_n )))/n_train


    slope_trend.append(loss_train)
    loss_train_trend.append(loss_train)
    loss_test_trend.append(loss_test)
    loss_train_trend_n.append(loss_train_n)
    loss_test_trend_n.append(loss_test_n)
    print("")

    print(f" Iteration {i+1}, slope a1 = {derivative_a1}, slope a2 = {derivative_a2}, slope a4 = {derivative_a4}, \
     slope a5 = {derivative_a5}, slope a7 = {derivative_a7}, slope b = {derivative_b}, loss-train = {loss_train}, loss-test = {loss_test}")

    print("")

    print(f" Iteration {i+1}, slope a1_n = {derivative_a1_n}, slope a2_n = {derivative_a2_n}, slope a4_n = {derivative_a4_n}, \
     slope a5_n = {derivative_a5_n}, slope a7_n = {derivative_a7_n}, slope b_n = {derivative_b_n}, loss-train_n = {loss_train_n}, loss-test_n = {loss_test_n}")

    if abs(derivative_a1) < 10**-6  or abs(derivative_b) < 10**-6 or i > 0 and abs(slope_trend[i-1] - slope_trend[i]) < 0.0001:
      break

    else:
      step_a1 = derivative_a1 * learning_rate
      a1 = a1 - step_a1

      step_a2 = derivative_a2 * learning_rate
      a2 = a2 - step_a2

      #step_a3 = derivative_a3 * learning_rate
      #a3 = a3 - step_a3

      step_a4 = derivative_a4 * learning_rate
      a4 = a4 - step_a4

      step_a5 = derivative_a5 * learning_rate
      a5 = a5 - step_a5

      #step_a6 = derivative_a6 * learning_rate
      #a6 = a6 - step_a6

      step_a7 = derivative_a7 * learning_rate
      a7 = a7 - step_a7

      #step_a8 = derivative_a8 * learning_rate
      #a8 = a8 - step_a8

      #step_a9 = derivative_a9 * learning_rate
      #a9 = a9 - step_a9

      #step_a10 = derivative_a10 * learning_rate
      #a10 = a10 - step_a10


      step_b = derivative_b * learning_rate
      b = b - step_b



  #derivativeb is slope of (sum squared residual vs intercept)
  print(f"a1 is: {a1}")
  print(f"a2 is: {a2}")
  #print(f"a3 is: {a3}")
  print(f"a4 is: {a4}")
  print(f"a5 is: {a5}")
  #print(f"a6 is: {a6}")
  print(f"a7 is: {a7}")
  #print(f"a8 is: {a8}")
  #print(f"a9 is: {a9}")
  #print(f"a10 is: {a10}")
  print(f"b is: {b}")

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend[::-1], marker="o", label="Training Loss", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend[::-1], marker="o", label="Testing Loss", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function')
  plt.title('Training and Testing Loss Trend')
  plt.legend()
  plt.show()

  #Plot the trend of difference between iteration and derivativea
  plt.plot(range(i+1, 0, -1), loss_train_trend_n[::-1], marker="o", label="Training Loss n", color="blue")
  plt.plot(range(i+1, 0, -1), loss_test_trend_n[::-1], marker="o", label="Testing Loss n", color="orange")
  plt.xlabel('Iteration')
  plt.ylabel('Loss Function_n')
  plt.title('Training and Testing Loss Trend_n')
  plt.legend()
  plt.show()

gradient_descent(b=10, a1=10, a2=10, a4=10, a5=10, a7=10, iteration=20, learning_rate=0.1)

"""#Normalization doesn't have effect on convergence and as you see, it slope are the same. But It made the quantity of the numbers bigger"""